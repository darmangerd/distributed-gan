\chapter{Methodology}
\label{chap:methodology}

\section{Distributed architectures}

\section{MD-GAN Algorithm}
\begin{algorithm}[ht]
    \caption{MD-GAN algorithm}
    \begin{algorithmic}[1]
    \Procedure{Worker}{$C, B_n, I, L, b$}
        \State Initialize $\theta_n$ for $D_n$
        \For{$i \gets 1$ to $I$}
            \State $X_n^{(r)} \gets \text{SAMPLES}(B_n)$
            \State $(X_n^{(g)}, X_n^{(d)}) \gets \text{RECEIVE\_BATCHES}(C)$
            \For{$l \gets 0$ to $L$}
                \State $D_n \gets \text{LEARNING\_STEP}(J_{\text{disc}}, D_n)$
            \EndFor
            \State $F_n \gets \{\frac{\partial \tilde{B}(X_n^(g))}{\partial x_i} \mid x_i \in X_n^{(g)} \}$
            \State \text{SEND}(C, $F_n$)
            \If{$i \mod \frac{mE}{b} = 0$}
                \State $D_n \gets \text{SWAP}(D_n)$
            \EndIf
        \EndFor
    \EndProcedure
    \\
    \Procedure{Swap}{$D_n$}
        \State $W_l \gets \text{GET\_RANDOM\_WORKER}()$
        \State \text{SEND}($W_l, D_n$)
        \State $D_n \gets \text{RECEIVE\_DISCRIMINATOR}()$
        \State \Return $D_n$
    \EndProcedure
    \\
    \Procedure{Server}{$k, I$}
        \State Initialize $w$ for $G$
        \For{$i \gets 1$ to $I$}
            \If{$\text{RECEIVED\_SWAP\_REQUEST}$}
                \State $w \gets \text{SWAP\_GENERATOR}(w)$
            \EndIf
            \For{$j \gets 0$ to $k$}
                \State $Z_j \gets \text{GAUSSIANNOISE}(b)$
                \State $X_j^{(g)} \gets \{G_w(z) \mid z \in Z_j\}$
            \EndFor
            \State $(X_1^{(d)}, \dots, X_n^{(d)}) \gets \text{SPLIT}(X_1, \dots, X_k)$
            \State $(X_1^{(g)}, \dots, X_n^{(g)}) \gets \text{SPLIT}(X_1, \dots, X_k)$
            \For{$n \gets 1$ to $N$}
                \State \text{SEND}($w_n, (X_n^{(d)}, X_n^{(g)})$)
            \EndFor
        \EndFor
        \State $F_1, \dots, F_N \gets \text{GET\_FEED\_BACK\_FROM\_WORKERS}()$
        \State Compute $\Delta w$ according to $F_1, \dots, F_N$
        \For{$w_i \in w$}
            \State $w_i \gets w_i + \text{ADAM}(\Delta w_i)$
        \EndFor
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{table}[ht]
    \centering
    \caption{Notation for MD-GAN algorithm}
    \begin{tabular}{|c|m{10cm}|}
    \hline
    \textbf{Notation} & \textbf{Description} \\ \hline
    $G$ & Generator \\ \hline
    $D$ & Discriminator \\ \hline
    $N$ & Number of workers \\ \hline
    $C$ & Central server \\ \hline
    $W_n$ & Worker $n$ \\ \hline
    $P_{\text{data}}$ & Data distribution \\ \hline
    $P_G$ & Distribution of generator $G$ \\ \hline
    $w$ (resp. $\theta$) & Parameters of $G$ (resp. $D$) \\ \hline
    $w_i$ (resp. $\theta_i$) & $i$-th parameter of $G$ (resp. $D$) \\ \hline
    $B$ & Distributed training dataset \\ \hline
    $B_n$ & Local training dataset on worker $n$ \\ \hline
    $m$ & Number of objects in a local dataset $B_n$ \\ \hline
    $d$ & Object size (e.g., image in Mb) \\ \hline
    $b$ & Batch size \\ \hline
    $I$ & Number of training iterations \\ \hline
    $K$ & The set of all batches $X(1), \dots, X(k)$ generated by $G$ during one iteration \\ \hline
    $F_n$ & The error feedback computed by worker $n$ \\ \hline
    $E$ & Number of local epochs before swapping discriminators \\ \hline
    $J_{\text{disc}}$ & Discriminator loss function $J_{\text{disc}}(X_r, X_g) = \tilde{A}(X_r) + \tilde{B}(X_g)$ \\ \hline
    $\tilde{A}$ & Discriminator loss on real data $\tilde{A} = \frac{1}{b} \sum_{x_i \in X_r} \log D_{\theta}(x_i)$ \\ \hline
    $\tilde{B}$ & Discriminator loss on generated data $\tilde{B} = \frac{1}{b} \sum_{x_i \in X_g} \log (1 - D_{\theta}(x_i))$ \\ \hline
    \end{tabular}
\end{table}
