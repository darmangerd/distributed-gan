\chapter{Related works and fundamentals}

\section{Generative Adversarial Networks}
Generative Adversarial Networks (GANs) are a class of generative models that have gained popularity in recent years. Introduced by Goodfellow et al. in 2014 \cite{goodfellow2014generative}, GANs have been used in a wide range of applications, including image generation, style transfer, and data augmentation. The key idea behind GANs is to train two neural networks, a generator and a discriminator in a competing scenario where the generator learns to produce data that are indistinguishable from the real data, while the discriminator learns to distinguish between the generator's output and the real data. The two networks are trained simultaneously, with the generator trying to fool the discriminator and the discriminator trying to correctly classify the data. This adversarial training process leads to the generator learning the data distribution of the real data and generating realistic samples.

\section{Distributed Deep Learning}
Distributed Deep Learning is the process of training deep learning models across multiple computational nodes. This approach allows for the parallelization of the training process, leveraging the computational resources of multiple machines to scale the learning phase of deep learning models, to speed up the training phase, multi-instituational training and privacy (e.i. multiple could train a single model without sharing their data to each other directly). Distributing this task over multiple node is particularly useful for training large models on large datasets, where a single machine may not have enough memory or processing power to handle the training process (scaling). By distributing the training process across multiple nodes, deep learning models can be trained more efficiently and at a larger scale. There are several challenges associated with distributed deep learning, including communication overhead, synchronization issues, and load balancing. However, with the right architecture and optimization techniques, distributed deep learning can significantly improve privacy, scalability, and efficiency.

\section{Multi-Discriminator GANs}
Multi-Discriminator GANs (MD-GANs) are a variant of GANs that use multiple discriminators to improve the training process. The GAN architecture typically consists of a single discriminator that learns to distinguish between the generator's output and the real data. In contrast, MD-GANs use multiple discriminators \cite{mdgan}, each of which is responsible for a different aspect of the data distribution. By training multiple discriminators in parallel, MD-GANs can capture more fine-grained details of the data distribution and generate more realistic samples. Additionally, MD-GANs can be trained in a distributed setting, where each discriminator accesses only its local data. This setup enhances privacy by avoiding the centralization of sensitive data and allows for the training of GANs on large, distributed datasets.
