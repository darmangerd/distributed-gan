\chapter{Scientific Questions and Hypothesis}
\section{Scientific questions}
In distributed deep learning systems, especially those involving Generative Adversarial Networks, several important scientific questions emerge. These questions help us understand the benefits and challenges of implementing GANs across distributed settings.

\subsection*{A) Does training GANs in a distributed setting helps converging to an optimal solution in less epochs?} \label{que:a}
In distributed GAN settings, multiple computational nodes collaborate to train the model. This arrangement potentially allows the model to simultaneously learn more diverse features from different data subsets. We aim to determine if this setup leads to faster convergence compared to traditional, centralized training methods, where the model learns from a single data source. It can also be the opposite, as for a crowd where people would not listen to each other, people could contradict each other, canceling out what the other says.

% \subsection*{B) Can distributed training of GANs lead to a superior final model?}
% Leveraging the computational power and data variety from multiple sources, distributed GANs might not only accelerate the training process but also enhance the quality of the generated outputs. This question explores whether the final model, trained across multiple nodes, produces more accurate and realistic results than those trained on a centralized system.

\subsection*{B) Does Training GANs in a Distributed Setting Help Achieve an Epoch in Less Time?} \label{que:b}
It might be intuitive to assume that training a GAN with two workers would be twice as fast as in a standalone setting. However, this question requires a more detailed examination. In a distributed setting, multiple factors come into play that can affect the overall training time.

Firstly, the communication overhead between the server and the workers can significantly impact the training speed. Each worker needs to send and receive data batches, gradients, and other information, which introduces latency. Secondly, the synchronization of gradients across all workers can become a bottleneck, especially as the number of workers increases. This is because all workers must wait until the slowest worker finishes its computation before proceeding to the next iteration.

Moreover, the efficiency of distributed training also depends on the network bandwidth and the efficiency of the implementation of the distributed system. If the network is slow or the implementation is not optimized, the expected speedup may not be realized.

In summary, while distributed training of GANs has the potential to reduce the time per epoch by leveraging parallelism, the actual speedup achieved can be influenced by communication overhead, synchronization issues, network performance, and the effectiveness of workload distribution. Therefore, achieving a linear speedup, where two workers perform twice as fast as one, is not always guaranteed and requires careful consideration of these factors.

% \subsection*{C) Does distributed training of GANs offer enhanced privacy protection?)} 
% Distributed learning allows models to be trained directly on data sources without the need for centralization. This method minimizes the risk of exposing sensitive data, thereby potentially enhancing privacy protections.
% \textit{Je pense que c'est mieux de le dire dans results, en mode : oui les r√©sultats sont pas mieux mais par contre on gagne en terme de privacy donc Trade-off, Tu peux me supprimer quand tu m'as lu \^\_\^ uwu}

\newpage

\section{Hypothesis}
Building on the scientific questions, we formulate testable hypotheses that will guide the experimental phase of our project. These hypotheses provide  predictions that relate directly to the potential advantages of distributed GAN training.

\subsection{Hypothesis on Convergence Speed}
\begin{itemize}
    \item \textbf{Statement}: Distributed training of GANs converges faster to an optimal solution than centralized training methods.
    \item \textbf{Reasoning}: Distributed systems enable parallel processing of data across multiple nodes, which can lead to more efficient exploration of the solution space. With multiple discriminators working with different data subsets, the learning process could be accelerated by aggregating diverse gradients that guide the generator more effectively.
\end{itemize}


\subsection{Hypothesis on Model Quality}
\begin{itemize}
    \item \textbf{Statement}: GANs trained in a distributed manner produce more accurate and realistic outputs than those trained in a centralized manner.
    \item \textbf{Reasoning}: Each discriminator in a distributed GAN framework learns from a unique subset of data, potentially leading to a more comprehensive understanding of the overall data distribution. This multi-perspective learning approach could result in a generator that creates higher-quality, more diverse data.
\end{itemize}

\subsection{Hypothesis on Privacy Protection}
\begin{itemize}
    \item \textbf{Statement}: Distributed training of GANs provides enhanced privacy protection compared to centralized training setups.
    \item \textbf{Reasoning}: Distributed training methodologies do not require the transfer of raw data between nodes, only model parameters and updates are exchanged. This minimizes the exposure of sensitive data, reducing the risk of data breaches. Furthermore, the separation of data across nodes can prevent any single point from reconstructing or accessing complete sensitive information.
\end{itemize}



\section{Summary}
These questions and hypotheses are fundamental to our research and will assist in evaluating the capability and effectiveness of distributed GANs. The empirical testing of these hypotheses through experiments will measure convergence speed, output quality, and privacy implications, contributing to a deeper understanding of the trade-offs and benefits of distributed GAN training.