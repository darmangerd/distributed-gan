\chapter{Experimental setup}
\section{Flower.ai first draft}
Flower.ai\footnote{\url{https://flower.ai/}} is a Python library that allows for the distributed training of machine learning models. It is designed to be framework-agnostic and can be used with any machine learning framework that supports the Python programming language, such as TensorFlow, PyTorch, and scikit-learn. Flower.ai provides a simple and intuitive API for distributed training, allowing users to easily scale their machine learning models across multiple devices and machines. However, in its current version Flower.ai enforces developpers to follow a strict pipeline for the training process. While implementing the MD-GAN, we found that the Flower.ai library was not flexible enough to accommodate the specific requirements of the MD-GAN architecture. We therefore decided to implement it using PyTorch's distributed package, which provides more control over the training process and allows for a more customized implementation.

\section{PyTorch distributed (over TCP)}
PyTorch's distributed package offers crucial capabilities for sending and receiving tensors across various processes. We selected the TCP backend within this framework to enable robust network communication between machines. This setup allows us to efficiently send feedback to the central server and distribute generated images to the workers, facilitating effective and collaborative training of the GAN across multiple nodes. However we can't send dictionaries or complex objects, which are essential for the MD-GAN architecture, since we have to exchange the state of discriminators to avoid overfitting. We therefore decided to use TensorDict\footnote{\url{https://github.com/pytorch/tensordict}}, a module integrated into PyTorch, that turns dictionaries into tensors and vice versa. It is also capable of sending and receiving these tensors across different processes using the same TCP backend, so it fits perfectly with our requirements.

\section{Baseline comparison}
To evaluate the performance of our distributed MD-GAN implementation, we compared it to a baseline centralized implementation. The baseline implementation uses a single machine to train the GAN on the entire dataset, without any distribution of data or computation. This setup serves as a reference point for evaluating the benefits of distributed training and allows us to measure the impact of distributing the training process across multiple machines. We used the same hyperparameters and training settings for both the distributed and centralized implementations to ensure a fair comparison. It is important to note that the centralized implementation is requires to be run on a single machine, and therefore that data must be owned by the machine running the training process, which is not the case for the distributed implementation.

\section{Datasets used}
To evaluate our distributed MD-GAN implementation, we used two datasets: MNIST and CIFAR-10. The MNIST dataset consists of 60,000 grayscale images of handwritten digits with 10 classes (0-9). Each image is 28x28 pixels in size, making it a relatively small and simple dataset. The CIFAR-10 dataset, on the other hand, consists of 60,000 color images with 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). Each image is 32x32 pixels in size, making it a more complex and challenging dataset. By using these two datasets, we can evaluate the performance of our distributed MD-GAN implementation on both simple and complex data distributions, providing a comprehensive assessment of its effectiveness across different scenarios. We choosed these datasets because they were the ones used in the original MD-GAN paper, so we can compare our results with the ones obtained by the authors.

\section{Local setup}
It is possible to run the distributed MD-GAN implementation on a single machine, because every process is a separate Python process. This setup also enables us to run multiple workers on the same machine, a script called \texttt{run-with-server.sh} is provided to facilitate starting the server and workers. This local setup is useful for testing and debugging the implementation before deploying it to a networked environment. It also allows us to evaluate the performance of the distributed MD-GAN on a small scale, providing insights into how the system behaves under different conditions. We used this setup to test the implementation on the MNIST and CIFAR-10 datasets, as well as to verify the correctness of the communication between the server and workers.

\section{Networking}


\section{Google Cloud setup}